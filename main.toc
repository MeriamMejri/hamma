\thispagestyle {MyStyle}
\contentsline {chapter}{Liste des abréviations}{ix}{chapter*.4}%
\contentsline {chapter}{Liste des figures}{xii}{chapter*.5}%
\contentsline {chapter}{Liste des tables}{xii}{chapter*.5}%
\contentsline {chapter}{INTRODUCTION \MakeUppercase []{générale}}{1}{chapter*.6}%
\contentsline {chapter}{\numberline {1}Présentation générale du projet}{3}{chapter.1}%
\contentsline {section}{\numberline {1.1}Introduction}{4}{section.1.1}%
\contentsline {section}{\numberline {1.2}Présentation du lieu du stage}{4}{section.1.2}%
\contentsline {subsection}{\numberline {1.2.1}Présentation de l’entreprise}{4}{subsection.1.2.1}%
\contentsline {subsection}{\numberline {1.2.2}Les Valeurs de l’entreprise}{5}{subsection.1.2.2}%
\contentsline {subsection}{\numberline {1.2.3}La vision de l’entreprise}{6}{subsection.1.2.3}%
\contentsline {subsection}{\numberline {1.2.4}L’organigramme de l’entreprise}{6}{subsection.1.2.4}%
\contentsline {subsection}{\numberline {1.2.5}L’organigramme de la direction Technologie }{7}{subsection.1.2.5}%
\contentsline {section}{\numberline {1.3}Contexte générale du projet}{8}{section.1.3}%
\contentsline {subsection}{\numberline {1.3.1}Problématique}{8}{subsection.1.3.1}%
\contentsline {subsection}{\numberline {1.3.2}Objectif du projet}{10}{subsection.1.3.2}%
\contentsline {subsection}{\numberline {1.3.3}Méthodologie de travail}{10}{subsection.1.3.3}%
\contentsline {subsection}{\numberline {1.3.4}Choix technologiques}{11}{subsection.1.3.4}%
\contentsline {section}{\numberline {1.4}Concepts Clés}{12}{section.1.4}%
\contentsline {subsection}{\numberline {1.4.1}Data Mining}{12}{subsection.1.4.1}%
\contentsline {subsection}{\numberline {1.4.2}Machine Learning}{13}{subsection.1.4.2}%
\contentsline {subsection}{\numberline {1.4.3}Data Mining vs Machine Learning}{14}{subsection.1.4.3}%
\contentsline {subsection}{\numberline {1.4.4}Deep Learning}{15}{subsection.1.4.4}%
\contentsline {subsubsection}{\numberline {1.4.4.1}Principes du Deep Learning}{15}{subsubsection.1.4.4.1}%
\contentsline {subsubsection}{\numberline {1.4.4.2}Applications du Deep Learning}{16}{subsubsection.1.4.4.2}%
\contentsline {subsubsection}{\numberline {1.4.4.3}Différence entre Machine Learning et Deep Learning}{16}{subsubsection.1.4.4.3}%
\contentsline {section}{\numberline {1.5}Conclusion}{17}{section.1.5}%
\contentsline {chapter}{\numberline {2}Outils mathématiques d'ordre général}{18}{chapter.2}%
\contentsline {section}{\numberline {2.1}Introduction}{20}{section.2.1}%
\contentsline {section}{\numberline {2.2}Techniques de Prétraitement des Données}{20}{section.2.2}%
\contentsline {subsection}{\numberline {2.2.1}Normalisation des Données}{20}{subsection.2.2.1}%
\contentsline {subsubsection}{\numberline {2.2.1.1}Méthodes classiques de normalisation}{20}{subsubsection.2.2.1.1}%
\contentsline {subsubsection}{\numberline {2.2.1.2}Méthodes avancées de normalisation}{21}{subsubsection.2.2.1.2}%
\contentsline {subsection}{\numberline {2.2.2}Traitement des Valeurs Manquantes}{22}{subsection.2.2.2}%
\contentsline {subsubsection}{\numberline {2.2.2.1}Imputation par la Médiane}{22}{subsubsection.2.2.2.1}%
\contentsline {subsubsection}{\numberline {2.2.2.2}K-Nearest Neighbors pour la Prédiction des Valeurs Manquantes}{22}{subsubsection.2.2.2.2}%
\contentsline {subsection}{\numberline {2.2.3}Conclusion}{23}{subsection.2.2.3}%
\contentsline {section}{\numberline {2.3}Les indicateurs statistiques}{23}{section.2.3}%
\contentsline {subsection}{\numberline {2.3.1}Eta Carré}{24}{subsection.2.3.1}%
\contentsline {subsection}{\numberline {2.3.2}Cramer V}{24}{subsection.2.3.2}%
\contentsline {subsection}{\numberline {2.3.3}Lambda de Goodman et Kruskal}{25}{subsection.2.3.3}%
\contentsline {subsection}{\numberline {2.3.4}Covariance}{25}{subsection.2.3.4}%
\contentsline {subsection}{\numberline {2.3.5}Limitation des Indicateurs Statistiques}{26}{subsection.2.3.5}%
\contentsline {subsection}{\numberline {2.3.6}Conclusion}{26}{subsection.2.3.6}%
\contentsline {section}{\numberline {2.4}Les Tests de corrélation}{26}{section.2.4}%
\contentsline {subsection}{\numberline {2.4.1}Tests paramétriques}{27}{subsection.2.4.1}%
\contentsline {subsubsection}{\numberline {2.4.1.1}Les conditions pour appliquer les tests paramétriques}{27}{subsubsection.2.4.1.1}%
\contentsline {subsubsection}{\numberline {2.4.1.2}Test t de Student}{30}{subsubsection.2.4.1.2}%
\contentsline {subsubsection}{\numberline {2.4.1.3}Test ANOVA}{31}{subsubsection.2.4.1.3}%
\contentsline {subsubsection}{\numberline {2.4.1.4}Test de Pearson}{32}{subsubsection.2.4.1.4}%
\contentsline {subsection}{\numberline {2.4.2}Tests non paramétriques}{33}{subsection.2.4.2}%
\contentsline {subsubsection}{\numberline {2.4.2.1}Test U de Mann-Whitney}{33}{subsubsection.2.4.2.1}%
\contentsline {subsubsection}{\numberline {2.4.2.2}Test de Kruskal-Wallis}{33}{subsubsection.2.4.2.2}%
\contentsline {subsubsection}{\numberline {2.4.2.3}Test de Spearman}{34}{subsubsection.2.4.2.3}%
\contentsline {subsubsection}{\numberline {2.4.2.4}Test du Chi-Deux}{34}{subsubsection.2.4.2.4}%
\contentsline {subsubsection}{\numberline {2.4.2.5}Test exact de Fisher}{35}{subsubsection.2.4.2.5}%
\contentsline {section}{\numberline {2.5}Équilibrage des Classes avec SMOTE}{36}{section.2.5}%
\contentsline {section}{\numberline {2.6}Sélection des Caractéristiques (Feature Selection)}{37}{section.2.6}%
\contentsline {subsection}{\numberline {2.6.1}RFE (Recursive Feature Elimination)}{37}{subsection.2.6.1}%
\contentsline {subsection}{\numberline {2.6.2}SelectKBest}{38}{subsection.2.6.2}%
\contentsline {subsection}{\numberline {2.6.3}Feature Importance avec Forêt Aléatoire}{38}{subsection.2.6.3}%
\contentsline {subsection}{\numberline {2.6.4}Conclusion}{39}{subsection.2.6.4}%
\contentsline {section}{\numberline {2.7}Modèles de Machine Learning}{39}{section.2.7}%
\contentsline {subsection}{\numberline {2.7.1}Régression Logistique}{39}{subsection.2.7.1}%
\contentsline {subsubsection}{\numberline {2.7.1.1}Hyperparamètres}{41}{subsubsection.2.7.1.1}%
\contentsline {subsection}{\numberline {2.7.2}L’Arbre de Décision}{43}{subsection.2.7.2}%
\contentsline {subsection}{\numberline {2.7.3}La Forêt Aléatoire: un modèle non paramétrique}{45}{subsection.2.7.3}%
\contentsline {subsection}{\numberline {2.7.4}AdaBoost: Amélioration des Arbres de Décision}{46}{subsection.2.7.4}%
\contentsline {subsection}{\numberline {2.7.5}Conclusion}{48}{subsection.2.7.5}%
\contentsline {section}{\numberline {2.8}Deep Learning}{48}{section.2.8}%
\contentsline {subsection}{\numberline {2.8.1}Les Couches dans un Réseau de Neurones}{48}{subsection.2.8.1}%
\contentsline {subsection}{\numberline {2.8.2}Fonctions d'Activation}{49}{subsection.2.8.2}%
\contentsline {subsection}{\numberline {2.8.3}Entraînement et Backpropagation}{49}{subsection.2.8.3}%
\contentsline {subsubsection}{\numberline {2.8.3.1}Calcul de la Fonction de Perte}{49}{subsubsection.2.8.3.1}%
\contentsline {subsubsection}{\numberline {2.8.3.2}Backpropagation et Descente de Gradient}{50}{subsubsection.2.8.3.2}%
\contentsline {subsection}{\numberline {2.8.4}Régularisation et Dropout}{50}{subsection.2.8.4}%
\contentsline {subsection}{\numberline {2.8.5}Hyperparamètres: Signification et Impact}{51}{subsection.2.8.5}%
\contentsline {subsection}{\numberline {2.8.6}Conclusion}{51}{subsection.2.8.6}%
\contentsline {section}{\numberline {2.9}Interprétation des Modèles pour la Prédiction de la Satisfaction Client}{51}{section.2.9}%
\contentsline {subsection}{\numberline {2.9.1}Clustering avec KMeans}{51}{subsection.2.9.1}%
\contentsline {subsection}{\numberline {2.9.2}Calcul de la Probabilité de Satisfaction}{52}{subsection.2.9.2}%
\contentsline {subsection}{\numberline {2.9.3}Interprétation des Résultats}{52}{subsection.2.9.3}%
\contentsline {subsection}{\numberline {2.9.4}Conclusion}{52}{subsection.2.9.4}%
\contentsline {section}{\numberline {2.10}Conclusion}{52}{section.2.10}%
\contentsline {chapter}{\numberline {3}Réalisation}{53}{chapter.3}%
\contentsline {section}{\numberline {3.1}Introduction}{54}{section.3.1}%
\contentsline {section}{\numberline {3.2}Prétraitement des données}{54}{section.3.2}%
\contentsline {subsection}{\numberline {3.2.1}Préparation et nettoyage des données}{54}{subsection.3.2.1}%
\contentsline {subsection}{\numberline {3.2.2}Normalisation des données}{58}{subsection.3.2.2}%
\contentsline {subsection}{\numberline {3.2.3}Utilisation de plusieurs tables}{58}{subsection.3.2.3}%
\contentsline {section}{\numberline {3.3}Analyse des données}{59}{section.3.3}%
\contentsline {subsection}{\numberline {3.3.1}Analyse descriptive}{59}{subsection.3.3.1}%
\contentsline {subsubsection}{\numberline {3.3.1.1}Analyse des variables quantitatives}{60}{subsubsection.3.3.1.1}%
\contentsline {subsubsection}{\numberline {3.3.1.2}Analyse des variables qualitative}{63}{subsubsection.3.3.1.2}%
\contentsline {subsubsection}{\numberline {3.3.1.3}Comparaison globale entre OSAT multiclasse et OSAT binaire}{66}{subsubsection.3.3.1.3}%
\contentsline {subsection}{\numberline {3.3.2}Analyse inférentielle}{66}{subsection.3.3.2}%
\contentsline {subsubsection}{\numberline {3.3.2.1}Corrélation entre la variable cible et les variables explicatives}{67}{subsubsection.3.3.2.1}%
\contentsline {subsubsection}{\numberline {3.3.2.2}Analyse des corrélations entre les variables explicatives}{72}{subsubsection.3.3.2.2}%
\contentsline {section}{\numberline {3.4}Modélisation}{76}{section.3.4}%
\contentsline {subsection}{\numberline {3.4.1}Équilibrage des classes (SMOTE)}{76}{subsection.3.4.1}%
\contentsline {subsection}{\numberline {3.4.2}Sélection des caractéristiques}{78}{subsection.3.4.2}%
\contentsline {subsection}{\numberline {3.4.3}Modélisation avec Machine Learning}{80}{subsection.3.4.3}%
\contentsline {subsubsection}{\numberline {3.4.3.1}Introduction}{80}{subsubsection.3.4.3.1}%
\contentsline {subsubsection}{\numberline {3.4.3.2}Prétraitement des données}{80}{subsubsection.3.4.3.2}%
\contentsline {subsubsection}{\numberline {3.4.3.3}Optimisation des hyperparamètres}{80}{subsubsection.3.4.3.3}%
\contentsline {subsubsection}{\numberline {3.4.3.4}Sélection des caractéristiques importantes et réentraînement du modèle}{80}{subsubsection.3.4.3.4}%
\contentsline {subsubsection}{\numberline {3.4.3.5}Optimisation du seuil de classification}{81}{subsubsection.3.4.3.5}%
\contentsline {subsubsection}{\numberline {3.4.3.6}Conclusion}{81}{subsubsection.3.4.3.6}%
\contentsline {subsection}{\numberline {3.4.4}Modélisation avec Deep Learning}{81}{subsection.3.4.4}%
\contentsline {subsubsection}{\numberline {3.4.4.1}Approche PyTorch}{81}{subsubsection.3.4.4.1}%
\contentsline {subsubsection}{\numberline {3.4.4.2}Approche Keras/TensorFlow}{82}{subsubsection.3.4.4.2}%
\contentsline {subsubsection}{\numberline {3.4.4.3}Conclusion}{82}{subsubsection.3.4.4.3}%
\contentsline {section}{\numberline {3.5}Conclusion}{83}{section.3.5}%
\contentsline {chapter}{\numberline {4}Résultats et interprétation}{84}{chapter.4}%
\contentsline {section}{\numberline {4.1}Introduction}{85}{section.4.1}%
\contentsline {section}{\numberline {4.2}Résultats des Modèles de Machine Learning}{85}{section.4.2}%
\contentsline {subsection}{\numberline {4.2.1}Modèle de Régression Logistique}{85}{subsection.4.2.1}%
\contentsline {subsubsection}{\numberline {4.2.1.1}Résultats pour les trois datasets}{85}{subsubsection.4.2.1.1}%
\contentsline {subsubsection}{\numberline {4.2.1.2}Comparaison des résultats entre les trois datasets}{86}{subsubsection.4.2.1.2}%
\contentsline {subsection}{\numberline {4.2.2}Modèle Arbre de Décision}{87}{subsection.4.2.2}%
\contentsline {subsubsection}{\numberline {4.2.2.1}Résultats pour les trois datasets}{87}{subsubsection.4.2.2.1}%
\contentsline {subsubsection}{\numberline {4.2.2.2}Comparaison des résultats entre les trois datasets}{89}{subsubsection.4.2.2.2}%
\contentsline {subsection}{\numberline {4.2.3}Modèle de Random Forest}{89}{subsection.4.2.3}%
\contentsline {subsubsection}{\numberline {4.2.3.1}Résultats pour les trois datasets}{89}{subsubsection.4.2.3.1}%
\contentsline {subsubsection}{\numberline {4.2.3.2}Sélection automatique du seuil d'importance}{91}{subsubsection.4.2.3.2}%
\contentsline {subsubsection}{\numberline {4.2.3.3}Approche de KMeans et résultats pour \textit {retail\_juin}}{92}{subsubsection.4.2.3.3}%
\contentsline {subsubsection}{\numberline {4.2.3.4}Résultats après KMeans sur \textit {retail\_juin}}{93}{subsubsection.4.2.3.4}%
\contentsline {subsubsection}{\numberline {4.2.3.5}Conclusion}{94}{subsubsection.4.2.3.5}%
\contentsline {section}{\numberline {4.3}Résultats des Modèles de Deep Learning}{95}{section.4.3}%
\contentsline {subsection}{\numberline {4.3.1}Modèle Deep Learning avec PyTorch}{95}{subsection.4.3.1}%
\contentsline {subsubsection}{\numberline {4.3.1.1}Comparaison des résultats entre les trois datasets}{96}{subsubsection.4.3.1.1}%
\contentsline {subsection}{\numberline {4.3.2}Résultats des Modèles avec Keras/TensorFlow}{97}{subsection.4.3.2}%
\contentsline {subsubsection}{\numberline {4.3.2.1}Résultats pour \textit {network\_fev}}{97}{subsubsection.4.3.2.1}%
\contentsline {subsubsection}{\numberline {4.3.2.2}Résultats pour \textit {network\_may}}{98}{subsubsection.4.3.2.2}%
\contentsline {subsubsection}{\numberline {4.3.2.3}Résultats pour \textit {retail\_juin}}{98}{subsubsection.4.3.2.3}%
\contentsline {subsubsection}{\numberline {4.3.2.4}Comparaison des Résultats}{99}{subsubsection.4.3.2.4}%
\contentsline {section}{\numberline {4.4}Comparaison Entre les Modèles de Machine Learning et de Deep Learning}{99}{section.4.4}%
\contentsline {subsection}{\numberline {4.4.1}Comparaison et Validation des Modèles de Machine Learning}{99}{subsection.4.4.1}%
\contentsline {subsection}{\numberline {4.4.2}Comparaison et Validation des Modèles de Deep Learning}{100}{subsection.4.4.2}%
\contentsline {subsection}{\numberline {4.4.3}Comparaison Globale : Machine Learning vs Deep Learning}{102}{subsection.4.4.3}%
\contentsline {subsection}{\numberline {4.4.4}Modèle d’Ensemble (Random Forest, AdaBoost et Réseau de Neurones)}{102}{subsection.4.4.4}%
\contentsline {subsubsection}{\numberline {4.4.4.1}Motivation de l’approche d’ensemble}{103}{subsubsection.4.4.4.1}%
\contentsline {subsubsection}{\numberline {4.4.4.2}Justification des datasets sélectionnés}{103}{subsubsection.4.4.4.2}%
\contentsline {subsubsection}{\numberline {4.4.4.3}Optimisation des poids pour la combinaison des modèles}{104}{subsubsection.4.4.4.3}%
\contentsline {subsubsection}{\numberline {4.4.4.4}Résultats pour \textit {network\_fev}}{104}{subsubsection.4.4.4.4}%
\contentsline {subsubsection}{\numberline {4.4.4.5}Résultats pour \textit {retail\_juin}}{105}{subsubsection.4.4.4.5}%
\contentsline {subsection}{\numberline {4.4.5}Vérification de surajustement du meilleur modèle}{106}{subsection.4.4.5}%
\contentsline {subsection}{\numberline {4.4.6}Conclusion}{106}{subsection.4.4.6}%
\contentsline {section}{\numberline {4.5}Vérification du surajustement du meilleur modèle}{106}{section.4.5}%
\contentsline {subsection}{\numberline {4.5.1}Analyse des résultats}{107}{subsection.4.5.1}%
\contentsline {subsection}{\numberline {4.5.2}Conclusion sur le surajustement}{108}{subsection.4.5.2}%
\contentsline {section}{\numberline {4.6}Profil des Clients Satisfaits et Interprétation des Variables}{108}{section.4.6}%
\contentsline {subsection}{\numberline {4.6.1}Résultats pour \textit {network\_fev}}{108}{subsection.4.6.1}%
\contentsline {subsection}{\numberline {4.6.2}Résultats pour \textit {network\_may}}{109}{subsection.4.6.2}%
\contentsline {subsection}{\numberline {4.6.3}Résultats pour \textit {retail\_juin}}{110}{subsection.4.6.3}%
\contentsline {subsection}{\numberline {4.6.4}Comparaison des Résultats Entre les Trois Datasets}{111}{subsection.4.6.4}%
\contentsline {section}{\numberline {4.7}Conclusion}{112}{section.4.7}%
\contentsline {chapter}{Conclusion générale}{113}{chapter*.74}%
\contentsline {chapter}{Références}{113}{chapter*.74}%
\contentsline {chapter}{Annexe}{119}{chapter*.76}%
